{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from itertools import islice\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "\n",
    "COMET_API_KEY = \"mGG8ZYUSg6Hggq8qkPII0Vwx9\"\n",
    "PROJECT_NAME = \"pytorch-transformer-ts\"\n",
    "WORKSPACE = \"rolandriachi\"\n",
    "\n",
    "logger = CometLogger(api_key=COMET_API_KEY,\n",
    "                     project_name=PROJECT_NAME,\n",
    "                     workspace=WORKSPACE\n",
    "                    )\n",
    "\n",
    "from estimator import LagGPTEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDatasetIterator:\n",
    "    def __init__(self, datasets, seed, weights):\n",
    "        self._datasets = [iter(el) for el in datasets]\n",
    "        self._weights = weights\n",
    "        self._rng = random.Random(seed)\n",
    "\n",
    "    def __next__(self):\n",
    "        (dataset,) = self._rng.choices(self._datasets, weights=self._weights, k=1)\n",
    "        return next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset:\n",
    "    def __init__(self, datasets, seed=None, weights=None):\n",
    "        self._seed = seed\n",
    "        self._datasets = datasets\n",
    "        self._weights = weights\n",
    "        n_datasets = len(datasets)\n",
    "        if weights is None:\n",
    "            self._weights = [1 / n_datasets] * n_datasets\n",
    "\n",
    "    def __iter__(self):\n",
    "        return CombinedDatasetIterator(self._datasets, self._seed, self._weights)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum([len(ds) for ds in self._datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gluonts_ds = [\n",
    "        get_dataset(\"airpassengers\").train,\n",
    "        get_dataset(\"australian_electricity_demand\").train,\n",
    "        get_dataset(\"car_parts_without_missing\").train,\n",
    "        get_dataset(\"cif_2016\").train,\n",
    "        get_dataset(\"covid_deaths\").train,\n",
    "        get_dataset(\"electricity\").train,\n",
    "        get_dataset(\"electricity_weekly\").train,\n",
    "        get_dataset(\"exchange_rate\").train,\n",
    "        get_dataset(\"fred_md\").train,\n",
    "        get_dataset(\"hospital\").train,\n",
    "        get_dataset(\"kaggle_web_traffic_weekly\").train,\n",
    "        get_dataset(\"kdd_cup_2018_without_missing\").train,\n",
    "        get_dataset(\"london_smart_meters_without_missing\").train,\n",
    "        get_dataset(\"nn5_daily_with_missing\").train,\n",
    "        get_dataset(\"nn5_weekly\").train,\n",
    "        get_dataset(\"pedestrian_counts\").train,\n",
    "        get_dataset(\"rideshare_without_missing\").train,\n",
    "        get_dataset(\"saugeenday\").train,\n",
    "        get_dataset(\"solar-energy\").train,\n",
    "        get_dataset(\"solar_10_minutes\").train,\n",
    "        get_dataset(\"solar_weekly\").train,\n",
    "        get_dataset(\"taxi_30min\").train,\n",
    "        get_dataset(\"temperature_rain_without_missing\").train,\n",
    "        get_dataset(\"tourism_monthly\").train,\n",
    "        get_dataset(\"uber_tlc_daily\").train,\n",
    "        get_dataset(\"uber_tlc_hourly\").train,\n",
    "        get_dataset(\"vehicle_trips_without_missing\").train,\n",
    "        get_dataset(\"weather\").train,\n",
    "        get_dataset(\"wiki-rolling_nips\").train,\n",
    "        get_dataset(\"m4_daily\").train,\n",
    "        get_dataset(\"m4_hourly\").train,\n",
    "        get_dataset(\"m4_monthly\").train,\n",
    "        get_dataset(\"m4_quarterly\").train,\n",
    "        get_dataset(\"m4_yearly\").train,\n",
    "        get_dataset(\"wind_farms_without_missing\").train,\n",
    "]\n",
    "dataset = CombinedDataset(gluonts_ds, weights=[sum([len(x[\"target\"]) for x in d]) for d in gluonts_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = get_dataset(\"m4_weekly\").test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = get_dataset(\"m4_weekly\").metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LagGPTEstimator(\n",
    "    prediction_length=meta.prediction_length,\n",
    "    context_length=1024, # block_size: int = 2048 \n",
    "    batch_size=32, # 4\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=64, # 4096\n",
    "    scaling=\"std\",\n",
    "    num_batches_per_epoch=100,\n",
    "    trainer_kwargs=dict(max_epochs=100, accelerator=\"gpu\", precision=\"16\", logger=logger),\n",
    ")\n",
    "# Change num_batches_per_epoch <- 100 & max_epochs <- 100 & precision <- \"bf16-mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/r/roland.riachi/base/lib/python3.10/site-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/rolandriachi/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e\n",
      "\n",
      "/home/mila/r/roland.riachi/base/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/mila/r/roland.riachi/p4-time-series/lag-gpt/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-222aca38-70fe-52a4-82b3-dc4ae6e58916]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | LagGPTModel | 268 K \n",
      "--------------------------------------\n",
      "268 K     Trainable params\n",
      "0         Non-trainable params\n",
      "268 K     Total params\n",
      "1.074     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100it [00:59,  1.69it/s, v_num=6a8e]                            \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 60.12it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 55.04it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 50.84it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 49.85it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 49.64it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 49.83it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 49.21it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 49.57it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 50.26it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 50.92it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 51.52it/s]\u001b[A\n",
      "Epoch 0: : 100it [00:59,  1.68it/s, v_num=6a8e, val_loss=7.860]\n",
      "Epoch 0: : 100it [00:59,  1.68it/s, v_num=6a8e, val_loss=7.860, train_loss=-.605]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'val_loss' reached 7.85665 (best 7.85665), saving model to '/home/mila/r/roland.riachi/p4-time-series/lag-gpt/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e/checkpoints/epoch=0-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 100it [00:58,  1.70it/s, v_num=6a8e, val_loss=7.860, train_loss=-.605]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 52.09it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 49.72it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 47.41it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 46.71it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 46.52it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 46.72it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 46.18it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 46.46it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 47.08it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 47.70it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 48.46it/s]\u001b[A\n",
      "Epoch 1: : 100it [00:58,  1.70it/s, v_num=6a8e, val_loss=7.450, train_loss=-.605]\n",
      "Epoch 1: : 100it [00:58,  1.70it/s, v_num=6a8e, val_loss=7.450, train_loss=-.979]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 200: 'val_loss' reached 7.44821 (best 7.44821), saving model to '/home/mila/r/roland.riachi/p4-time-series/lag-gpt/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e/checkpoints/epoch=1-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: : 100it [01:01,  1.64it/s, v_num=6a8e, val_loss=7.450, train_loss=-.979]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 40.67it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 42.58it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 42.52it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 43.05it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 43.54it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 44.09it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 43.77it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 44.23it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 45.08it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 45.69it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 46.48it/s]\u001b[A\n",
      "Epoch 2: : 100it [01:01,  1.63it/s, v_num=6a8e, val_loss=7.330, train_loss=-.979]\n",
      "Epoch 2: : 100it [01:01,  1.63it/s, v_num=6a8e, val_loss=7.330, train_loss=-1.64]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 300: 'val_loss' reached 7.33234 (best 7.33234), saving model to '/home/mila/r/roland.riachi/p4-time-series/lag-gpt/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e/checkpoints/epoch=2-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: : 100it [01:00,  1.66it/s, v_num=6a8e, val_loss=7.330, train_loss=-1.64]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 59.28it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 54.58it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 50.37it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 49.38it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 49.22it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 49.42it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 48.29it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 48.70it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 49.44it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 50.15it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 50.86it/s]\u001b[A\n",
      "Epoch 3: : 100it [01:00,  1.66it/s, v_num=6a8e, val_loss=7.360, train_loss=-1.64]\n",
      "Epoch 3: : 100it [01:00,  1.66it/s, v_num=6a8e, val_loss=7.360, train_loss=-1.27]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 400: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: : 100it [00:59,  1.68it/s, v_num=6a8e, val_loss=7.360, train_loss=-1.27]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 58.06it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 34.13it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 29.10it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 27.38it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 26.54it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 26.11it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 25.62it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 25.47it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 25.46it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 25.48it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 26.88it/s]\u001b[A\n",
      "Epoch 4: : 100it [00:59,  1.67it/s, v_num=6a8e, val_loss=7.200, train_loss=-1.27]\n",
      "Epoch 4: : 100it [00:59,  1.67it/s, v_num=6a8e, val_loss=7.200, train_loss=-1.49]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 500: 'val_loss' reached 7.20162 (best 7.20162), saving model to '/home/mila/r/roland.riachi/p4-time-series/lag-gpt/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e/checkpoints/epoch=4-step=500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: : 100it [01:01,  1.63it/s, v_num=6a8e, val_loss=7.200, train_loss=-1.49]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 60.13it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 55.05it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 50.67it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 49.72it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 49.57it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 49.77it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 49.13it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 49.48it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 50.18it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 50.84it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 51.72it/s]\u001b[A\n",
      "Epoch 5: : 100it [01:01,  1.63it/s, v_num=6a8e, val_loss=7.290, train_loss=-1.49]\n",
      "Epoch 5: : 100it [01:01,  1.63it/s, v_num=6a8e, val_loss=7.290, train_loss=-1.99]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 600: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: : 100it [01:02,  1.61it/s, v_num=6a8e, val_loss=7.290, train_loss=-1.99]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 44.22it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 31.25it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 27.65it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 27.02it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 26.52it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 26.64it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 26.04it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 25.83it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 26.06it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 26.01it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 27.42it/s]\u001b[A\n",
      "Epoch 6: : 100it [01:02,  1.60it/s, v_num=6a8e, val_loss=7.060, train_loss=-1.99]\n",
      "Epoch 6: : 100it [01:02,  1.60it/s, v_num=6a8e, val_loss=7.060, train_loss=-2.13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 700: 'val_loss' reached 7.06272 (best 7.06272), saving model to '/home/mila/r/roland.riachi/p4-time-series/lag-gpt/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e/checkpoints/epoch=6-step=700.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: : 100it [01:05,  1.52it/s, v_num=6a8e, val_loss=7.060, train_loss=-2.13]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 59.08it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 54.44it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 50.12it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 49.17it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 49.01it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 49.24it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 48.62it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 49.00it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 49.71it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 50.39it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 51.18it/s]\u001b[A\n",
      "Epoch 7: : 100it [01:06,  1.51it/s, v_num=6a8e, val_loss=7.050, train_loss=-2.13]\n",
      "Epoch 7: : 100it [01:06,  1.51it/s, v_num=6a8e, val_loss=7.050, train_loss=-2.03]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 800: 'val_loss' reached 7.04893 (best 7.04893), saving model to '/home/mila/r/roland.riachi/p4-time-series/lag-gpt/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e/checkpoints/epoch=7-step=800.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: : 100it [01:02,  1.59it/s, v_num=6a8e, val_loss=7.050, train_loss=-2.03]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: : 1it [00:00, 60.95it/s]\u001b[A\n",
      "Validation DataLoader 0: : 2it [00:00, 55.46it/s]\u001b[A\n",
      "Validation DataLoader 0: : 3it [00:00, 50.91it/s]\u001b[A\n",
      "Validation DataLoader 0: : 4it [00:00, 49.87it/s]\u001b[A\n",
      "Validation DataLoader 0: : 5it [00:00, 49.62it/s]\u001b[A\n",
      "Validation DataLoader 0: : 6it [00:00, 49.81it/s]\u001b[A\n",
      "Validation DataLoader 0: : 7it [00:00, 49.19it/s]\u001b[A\n",
      "Validation DataLoader 0: : 8it [00:00, 49.54it/s]\u001b[A\n",
      "Validation DataLoader 0: : 9it [00:00, 50.24it/s]\u001b[A\n",
      "Validation DataLoader 0: : 10it [00:00, 50.84it/s]\u001b[A\n",
      "Validation DataLoader 0: : 11it [00:00, 51.32it/s]\u001b[A\n",
      "Epoch 8: : 100it [01:03,  1.58it/s, v_num=6a8e, val_loss=7.040, train_loss=-2.03]\n",
      "Epoch 8: : 100it [01:03,  1.58it/s, v_num=6a8e, val_loss=7.040, train_loss=-1.98]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 900: 'val_loss' reached 7.03881 (best 7.03881), saving model to '/home/mila/r/roland.riachi/p4-time-series/lag-gpt/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e/checkpoints/epoch=8-step=900.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: : 69it [00:48,  1.43it/s, v_num=6a8e, val_loss=7.040, train_loss=-1.98] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml ExistingExperiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/rolandriachi/pytorch-transformer-ts/33900469071b4dcdb5e0904eafd46a8e\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [9] : (-2.1257266998291016, -0.6053866744041443)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [9]   : (7.0388102531433105, 7.8566508293151855)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : pytorch-lightning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     aug_prob                          : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     aug_rate                          : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss                              : beta=0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr                                : 0.001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/context_length       : 1024\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/distr_output         : gluonts.torch.distributions.studentT.StudentTOutput()\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/input_size           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/n_embd               : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/n_head               : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/n_layer              : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/num_parallel_samples : 100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/prediction_length    : 13\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model_kwargs/scaling              : std\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay                      : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 1 metrics, params and output messages\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter df (Tensor of shape (32, 1036)) of distribution Chi2() to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[2.9036, 2.8832, 2.8807,  ..., 2.2630, 2.2641, 2.2680],\n        [2.0980, 2.0980, 2.0980,  ..., 2.0980, 2.0980, 2.0980],\n        [2.2346, 2.2461, 2.2408,  ..., 2.5038, 2.2733, 2.3083],\n        ...,\n        [2.2664, 2.2687, 2.2706,  ..., 2.2369, 2.2398, 2.2336],\n        [2.0980, 2.0980, 2.0980,  ..., 2.0980, 2.0980, 2.0980],\n        [2.0980, 2.0980, 2.0980,  ..., 2.0980, 2.0980, 2.0980]],\n       device='cuda:0', grad_fn=<MulBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gluonts/torch/model/estimator.py:237\u001b[0m, in \u001b[0;36mPyTorchLightningEstimator.train\u001b[0;34m(self, training_data, validation_data, shuffle_buffer_length, cache_data, ckpt_path, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     training_data: Dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchPredictor:\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gluonts/torch/model/estimator.py:205\u001b[0m, in \u001b[0;36mPyTorchLightningEstimator.train_model\u001b[0;34m(self, training_data, validation_data, from_predictor, shuffle_buffer_length, cache_data, ckpt_path, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m trainer_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    203\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_kwargs)\n\u001b[0;32m--> 205\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;241m.\u001b[39mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m best_model \u001b[38;5;241m=\u001b[39m training_network\u001b[38;5;241m.\u001b[39mload_from_checkpoint(\n\u001b[1;32m    214\u001b[0m     checkpoint\u001b[38;5;241m.\u001b[39mbest_model_path\n\u001b[1;32m    215\u001b[0m )\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:355\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:219\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:188\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         closure()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:266\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:146\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    149\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1270\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1234\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;124;03m    calls the optimizer.\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1270\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:161\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:231\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/amp.py:76\u001b[0m, in \u001b[0;36mMixedPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m skip_unscaling \u001b[38;5;241m=\u001b[39m closure_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:142\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:128\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 128\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:294\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 294\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    297\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:380\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/p4-time-series/lag-gpt/lightning_module.py:224\u001b[0m, in \u001b[0;36mLagGPTLightningModule.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_target\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture_target\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m freq_mix(\n\u001b[1;32m    221\u001b[0m             batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_target\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture_target\u001b[39m\u001b[38;5;124m\"\u001b[39m], rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug_rate\n\u001b[1;32m    222\u001b[0m         )\n\u001b[0;32m--> 224\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m     train_loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss\n",
      "File \u001b[0;32m~/p4-time-series/lag-gpt/lightning_module.py:192\u001b[0m, in \u001b[0;36mLagGPTLightningModule._compute_loss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    182\u001b[0m future_observed_reshaped \u001b[38;5;241m=\u001b[39m future_observed_values\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;241m*\u001b[39mfuture_observed_values\u001b[38;5;241m.\u001b[39mshape[extra_dims \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :],\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    187\u001b[0m distr_args, loc, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    188\u001b[0m     past_target\u001b[38;5;241m=\u001b[39mpast_target,\n\u001b[1;32m    189\u001b[0m     past_observed_values\u001b[38;5;241m=\u001b[39mpast_observed_values,\n\u001b[1;32m    190\u001b[0m     future_target\u001b[38;5;241m=\u001b[39mfuture_target_reshaped,\n\u001b[1;32m    191\u001b[0m )\n\u001b[0;32m--> 192\u001b[0m distr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistr_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistr_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m context_target \u001b[38;5;241m=\u001b[39m take_last(\n\u001b[1;32m    195\u001b[0m     past_target, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcontext_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    197\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    198\u001b[0m     (context_target, future_target_reshaped),\n\u001b[1;32m    199\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    200\u001b[0m )\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gluonts/torch/distributions/distribution_output.py:130\u001b[0m, in \u001b[0;36mDistributionOutput.distribution\u001b[0;34m(self, distr_args, loc, scale)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    111\u001b[0m     distr_args,\n\u001b[1;32m    112\u001b[0m     loc: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    113\u001b[0m     scale: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Distribution:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Construct the associated distribution, given the collection of\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    constructor arguments and, optionally, a scale tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m        batch_shape+event_shape of the resulting distribution.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     distr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistr_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distr\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gluonts/torch/distributions/distribution_output.py:107\u001b[0m, in \u001b[0;36mDistributionOutput._base_distribution\u001b[0;34m(self, distr_args)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m, distr_args):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistr_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdistr_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gluonts/torch/distributions/studentT.py:39\u001b[0m, in \u001b[0;36mStudentT.__init__\u001b[0;34m(self, df, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     34\u001b[0m     df: Union[\u001b[38;5;28mfloat\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m ):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/distributions/studentT.py:52\u001b[0m, in \u001b[0;36mStudentT.__init__\u001b[0;34m(self, df, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m broadcast_all(df, loc, scale)\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chi2 \u001b[38;5;241m=\u001b[39m \u001b[43mChi2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(batch_shape, validate_args\u001b[38;5;241m=\u001b[39mvalidate_args)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/distributions/chi2.py:24\u001b[0m, in \u001b[0;36mChi2.__init__\u001b[0;34m(self, df, validate_args)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, validate_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/distributions/gamma.py:54\u001b[0m, in \u001b[0;36mGamma.__init__\u001b[0;34m(self, concentration, rate, validate_args)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcentration\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/distributions/distribution.py:62\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 62\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter df (Tensor of shape (32, 1036)) of distribution Chi2() to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[2.9036, 2.8832, 2.8807,  ..., 2.2630, 2.2641, 2.2680],\n        [2.0980, 2.0980, 2.0980,  ..., 2.0980, 2.0980, 2.0980],\n        [2.2346, 2.2461, 2.2408,  ..., 2.5038, 2.2733, 2.3083],\n        ...,\n        [2.2664, 2.2687, 2.2706,  ..., 2.2369, 2.2398, 2.2336],\n        [2.0980, 2.0980, 2.0980,  ..., 2.0980, 2.0980, 2.0980],\n        [2.0980, 2.0980, 2.0980,  ..., 2.0980, 2.0980, 2.0980]],\n       device='cuda:0', grad_fn=<MulBackward0>)"
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(\n",
    "    training_data=dataset, \n",
    "    validation_data=val_dataset,\n",
    "    shuffle_buffer_length=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_dataset(\"traffic\").test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_dataset, predictor=predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list(forecast_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation: 100%|| 6034/6034 [00:02<00:00, 2316.62it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "agg_metrics, ts_metrics = evaluator(\n",
    "    iter(tss), iter(forecasts), num_series=len(test_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 100%|| 1000/1000 [00:01<00:00, 562.40it/s]\n",
      "/home/mila/r/roland.riachi/base/lib/python3.10/site-packages/gluonts/evaluation/_base.py:422: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  metrics[\"ND\"] = cast(float, metrics[\"abs_error\"]) / cast(\n",
      "/home/mila/r/roland.riachi/base/lib/python3.10/site-packages/gluonts/evaluation/_base.py:422: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  metrics[\"ND\"] = cast(float, metrics[\"abs_error\"]) / cast(\n",
      "/home/mila/r/roland.riachi/base/lib/python3.10/site-packages/gluonts/evaluation/_base.py:422: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  metrics[\"ND\"] = cast(float, metrics[\"abs_error\"]) / cast(\n",
      "/home/mila/r/roland.riachi/base/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:138: UserWarning: Warning: converting a masked element to nan.\n",
      "  return arr.astype(dtype, copy=True)\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "agg_metrics, ts_metrics = evaluator(\n",
    "    iter(tss[:1000]), iter(forecasts[:1000]), num_series=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.0015466555594824828,\n",
       " 'abs_error': 1.7559873163700104,\n",
       " 'abs_target_sum': 4.513999849557877,\n",
       " 'abs_target_mean': 0.06944615153165964,\n",
       " 'seasonal_error': 0.017650951088167655,\n",
       " 'MASE': 1.42741306854105,\n",
       " 'MAPE': 0.3416493562551645,\n",
       " 'sMAPE': 0.37925519943237307,\n",
       " 'MSIS': 19.54518595834432,\n",
       " 'QuantileLoss[0.1]': 0.6106239095330239,\n",
       " 'Coverage[0.1]': 0.13846153846153847,\n",
       " 'QuantileLoss[0.2]': 0.9888393431901932,\n",
       " 'Coverage[0.2]': 0.16923076923076924,\n",
       " 'QuantileLoss[0.3]': 1.2851681431755424,\n",
       " 'Coverage[0.3]': 0.21538461538461542,\n",
       " 'QuantileLoss[0.4]': 1.538294268772006,\n",
       " 'Coverage[0.4]': 0.2615384615384616,\n",
       " 'QuantileLoss[0.5]': 1.7559873824939132,\n",
       " 'Coverage[0.5]': 0.3230769230769231,\n",
       " 'QuantileLoss[0.6]': 1.9172822661697861,\n",
       " 'Coverage[0.6]': 0.3384615384615385,\n",
       " 'QuantileLoss[0.7]': 2.0307419935241344,\n",
       " 'Coverage[0.7]': 0.3384615384615385,\n",
       " 'QuantileLoss[0.8]': 2.0395083248615267,\n",
       " 'Coverage[0.8]': 0.4,\n",
       " 'QuantileLoss[0.9]': 1.824924959987402,\n",
       " 'Coverage[0.9]': 0.43076923076923074,\n",
       " 'RMSE': 0.03932754199644929,\n",
       " 'NRMSE': 0.5663026838646393,\n",
       " 'ND': 0.38900916590460244,\n",
       " 'wQuantileLoss[0.1]': 0.13527335619934314,\n",
       " 'wQuantileLoss[0.2]': 0.2190605618400818,\n",
       " 'wQuantileLoss[0.3]': 0.2847071745696709,\n",
       " 'wQuantileLoss[0.4]': 0.3407829685512006,\n",
       " 'wQuantileLoss[0.5]': 0.38900918055323,\n",
       " 'wQuantileLoss[0.6]': 0.4247413225672957,\n",
       " 'wQuantileLoss[0.7]': 0.44987639814011854,\n",
       " 'wQuantileLoss[0.8]': 0.4518184299588061,\n",
       " 'wQuantileLoss[0.9]': 0.4042811299974111,\n",
       " 'mean_absolute_QuantileLoss': 1.5545967324119476,\n",
       " 'mean_wQuantileLoss': 0.3443945024863509,\n",
       " 'MAE_Coverage': 0.22991452991452993,\n",
       " 'OWA': nan}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts_metrics.plot(x=\"MSIS\", y=\"MAPE\", kind=\"scatter\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    forecast.plot(color='g')\n",
    "    ts[-3 * 24:][0].plot(label=\"target\")\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
